{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26631095",
   "metadata": {},
   "source": [
    "Use browser drives: https://www.selenium.dev/documentation/webdriver/getting_started/install_drivers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6c386f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time      # time.sleep\n",
    "import json      # loading credentials\n",
    "import bs4 as bs # processing HTML retrieved from Selenium\n",
    "import re        # finding text by regular expressions\n",
    "import os.path   # checking if scrape csv file is present\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from unidecode import unidecode # normalize (รก -> a)superbid product name in order to create URL\n",
    "\n",
    "import pyotp                    # OTP functions\n",
    "import pandas as pd             # tabular data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0cd73b",
   "metadata": {},
   "source": [
    "## Use Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77411dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_credentials(source:str):\n",
    "    f = open('parameters.json')\n",
    "    return json.load(f)[source]\n",
    "\n",
    "def create_url_pars(base_url:str, query_params:dict) -> str:\n",
    "    query_pars = \"?\"\n",
    "    for k, v in query_params.items():\n",
    "        if v is not None and not k.startswith('__'):\n",
    "            query_pars += f'{k}={v}&'\n",
    "    if len(query_params) > 0:\n",
    "        query_pars = query_pars[:-1] # remove last &\n",
    "    else:\n",
    "        query_pars = \"\"\n",
    "    base_url = base_url + query_pars\n",
    "    return base_url\n",
    "        \n",
    "\n",
    "class Webscraper():\n",
    "    \n",
    "    TWITTER = 1\n",
    "    TWITTER_FILE = 'twitter_scrape.csv'\n",
    "    FACEBOOK_MP = 2\n",
    "    FACEBOOK_FILE = 'facebook_scrape.csv'\n",
    "    SUPERBID = 3\n",
    "    SUPERBID_FILE = 'superbid_scrape.csv'\n",
    "    \n",
    "    def __init__(self, scrape_location:int):\n",
    "        \"\"\"Scraper initializer.\n",
    "\n",
    "        Instantiate the Google Chrome browser, set parameters and \n",
    "        constants to be used in following methods, e.g., scrape location and scrape data file name.\n",
    "\n",
    "        Args:\n",
    "            scrape_location (int) : One of the integer class attributes.\n",
    "        \"\"\"\n",
    "        chrome_driver = ChromeService(ChromeDriverManager().install())\n",
    "#         options = Options()\n",
    "#         options.headless = True\n",
    "        chrome_options = webdriver.ChromeOptions() # this will disable image loading\n",
    "        chrome_options.add_argument('--blink-settings=imagesEnabled=false')\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        ## Attributes\n",
    "        self.driver = webdriver.Chrome(service=chrome_driver, options=chrome_options) # initialize the Chrome driver\n",
    "        self.WAITING_TIME = 5\n",
    "        self.wait = WebDriverWait(self.driver, self.WAITING_TIME) # wait maximum X seconds\n",
    "        self.scrape_location = scrape_location\n",
    "        if scrape_location == self.TWITTER:\n",
    "            self.scrape_file = self.TWITTER_FILE\n",
    "        elif scrape_location == self.FACEBOOK_MP:\n",
    "            self.scrape_file = self.FACEBOOK_FILE\n",
    "        elif scrape_location == self.SUPERBID:\n",
    "            self.scrape_file = self.SUPERBID_FILE\n",
    "    \n",
    "    def login(self) -> None:\n",
    "        \"\"\"Call according login function.\n",
    "\n",
    "        Go to login pages and perform needed steps to login, such as username, password and OTP.\n",
    "\n",
    "        Args:\n",
    "            scrape_location (int) : One of the integer class attributes.\n",
    "            \n",
    "        Raises:\n",
    "            NotImplementedError: In case self.scrape_location is not properly set.\n",
    "        \"\"\"\n",
    "        # head to twitter login page\n",
    "        if self.scrape_location == self.TWITTER:\n",
    "            credentials = get_credentials('twitter')\n",
    "            self.driver.get(\"https://twitter.com/i/flow/login\")\n",
    "            self.twitter_login(credentials['username'], credentials['password'], credentials['otp_key'])\n",
    "        elif self.scrape_location == self.FACEBOOK_MP:\n",
    "            parameters = get_credentials('facebook')            \n",
    "            credentials = parameters['credentials']\n",
    "            self.driver.get(\"https://www.facebook.com/marketplace\") # force login, have to reenter url afterwards\n",
    "            self.facebook_marketplace_login(credentials['username'], credentials['password'])\n",
    "        else:\n",
    "            raise NotImplementedError(\"Only Twitter and Facebook Marketplace logins are implemented by now\")\n",
    "            \n",
    "    def scrape(self, **kwargs:dict) -> None:\n",
    "        \"\"\"Call according scrape function.\n",
    "\n",
    "        Go to scrape pages and perform needed steps to retrieve needed data.\n",
    "\n",
    "        Args:\n",
    "            kwargs (dict) : Parameters passed down to scrape functions.\n",
    "            \n",
    "        Raises:\n",
    "            NotImplementedError: In case self.scrape_location is not properly set.\n",
    "        \"\"\"\n",
    "        if self.scrape_location == self.TWITTER:\n",
    "            print('Scraping Twitter Data...')\n",
    "            df = self.twitter_scrape(**kwargs)\n",
    "        elif self.scrape_location == self.FACEBOOK_MP:\n",
    "            print('Scraping Facebook Marketplace Data...')\n",
    "            df = self.facebook_marketplace_scrape(**kwargs)\n",
    "        elif self.scrape_location == self.SUPERBID:\n",
    "            print('Scraping Superbid Data...')\n",
    "            df = self.superbid_scrape(**kwargs)\n",
    "        else:\n",
    "            raise NotImplementedError(\"Only Twitter, Facebook Marketplace, and Superbid are implemented by now\")\n",
    "        print('Saving and Updating File...')\n",
    "        self.__update_and_save_scrape_file(df)\n",
    "        print('Done.')\n",
    "            \n",
    "    def __get_bet_end_date(self, bet_end_date_text:str):\n",
    "        \"\"\"Process string to retrieve proper bet end date.\n",
    "\n",
    "        Retrieve bet end date and create according datetime variable from multiple string formats.\n",
    "\n",
    "        Args:\n",
    "            bet_end_date_text (str) : Text extracted from HTML\n",
    "        \"\"\"\n",
    "        # 1st case example: \"Encerra em 1 dia\"\n",
    "        bet_end_tokens = bet_end_date_text.split(' ')\n",
    "        is_token = [str.isdigit(elem) for elem in bet_end_tokens]\n",
    "        is_token_true = [ix for ix,boolean in enumerate(is_token) if boolean]\n",
    "        bet_end_date_return = \"\"\n",
    "        if len(is_token_true) > 0:\n",
    "            days_to_end = int(bet_end_tokens[is_token_true[0]])\n",
    "            bet_end_date_return = (datetime.today() + timedelta(days=days_to_end)).strftime(\"%Y-%m-%d\")\n",
    "        elif ' - ' in bet_end_date_text : # 2nd case example: \"Encerra em 18/05 - 14:00\"\n",
    "            colon_index = bet_end_date_text.index(\":\")\n",
    "            hour_end = int(bet_end_date_text[colon_index-2:colon_index])\n",
    "            bar_index = bet_end_date_text.index(\"/\")\n",
    "            day_end = int(bet_end_date_text[bar_index-2:bar_index])\n",
    "            month_end = int(bet_end_date_text[bar_index+1:bar_index+3])\n",
    "            bet_end_datetime = datetime.strptime(f\"{datetime.now().year}-{month_end}-{day_end} {hour_end}:00\", \"%Y-%m-%d %H:%M\")\n",
    "            bet_end_date_return = bet_end_datetime.strftime(\"%Y-%m-%d\")\n",
    "        else: # 3rd case example: \"Encerra em 22h\"\n",
    "            hours_finish = int(re.search('(\\d+)h', bet_end_date_text).group(1))\n",
    "            bet_end_date_return = (datetime.today() + timedelta(hours=hours_finish)).strftime(\"%Y-%m-%d\")\n",
    "        return bet_end_date_return\n",
    "        \n",
    "    def __update_and_save_scrape_file(self, df:pd.DataFrame) -> None:\n",
    "        \"\"\"Update scrape data file with new scraped data.\n",
    "\n",
    "        Update csv file only with new retrieved data based on their IDs.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame) : Scraped data from ``self.scrape``.\n",
    "        \"\"\"\n",
    "        if os.path.isfile(self.scrape_file):\n",
    "            df_temp = pd.read_csv(self.scrape_file, index_col=0)\n",
    "            df_temp = pd.concat([df_temp, df])\n",
    "            df = df_temp.drop_duplicates(subset='id', keep='last') # keep most recent data\n",
    "        df.to_csv(self.scrape_file)\n",
    "        \n",
    "    def superbid_scrape(self, remove_duplicates:bool=True) -> pd.DataFrame:\n",
    "        \"\"\"Scrape data from superbid website.\n",
    "\n",
    "        Run through superbid data, extract HTML and extract fields.\n",
    "\n",
    "        Args:\n",
    "            remove_duplicates (bool) : Whether to remove possible retrieved duplicated fields.\n",
    "            \n",
    "        Returns:\n",
    "            Retrieved data frame.\n",
    "        \"\"\"\n",
    "        query_filters_list = get_credentials('superbid')['query_filters']\n",
    "        final_df = pd.DataFrame()\n",
    "        for query_filters in query_filters_list:\n",
    "            department = query_filters['department']\n",
    "            print(f'...Querying Department = {department}...')\n",
    "            del query_filters['department'] # it's not part of the pars url\n",
    "            query_filters['pageNumber'] = 1 #### fixed parameters\n",
    "            query_filters['pageSize'] = 60  ##################### \n",
    "            url = create_url_pars(f\"https://www.superbid.net/categorias/{department}\", query_filters)\n",
    "#             print(url) \n",
    "\n",
    "            self.driver.get(url)\n",
    "            self.wait.until(EC.element_to_be_clickable((By.XPATH, \"//div[@class='MuiGrid-root MuiGrid-container MuiGrid-item MuiGrid-grid-xs-12 MuiGrid-grid-sm-5 MuiGrid-grid-md-4 css-vx8qp']\")))\n",
    "\n",
    "            page_source = self.driver.page_source\n",
    "            soup = bs.BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "            # Get the items.\n",
    "            div = soup.find_all('div', class_='MuiGrid-root MuiGrid-container MuiGrid-item MuiGrid-grid-xs-12 MuiGrid-grid-sm-5 MuiGrid-grid-md-4 css-vx8qp')\n",
    "            product_names, urls, ids, prices, locations, bet_end_dates, img_sources, popularities = [], [], [], [], [], [], [], []\n",
    "            for d in div:\n",
    "                product_names.append(d['data-auction-name'])\n",
    "                ids.append(d['data-auction-id'])\n",
    "                prod_description = unidecode(d['data-auction-name']).lower()\n",
    "                prod_description = re.split('\\W+', prod_description)\n",
    "                prod_description = '-'.join(prod_description)\n",
    "                if prod_description[-1] == '-':\n",
    "                    prod_description = prod_description[:-1]\n",
    "                url = f\"https://www.superbid.net/oferta/{prod_description}-{d['data-auction-id']}\"\n",
    "                urls.append(url)\n",
    "                price = d.find_all(lambda tag: len(tag.find_all()) == 0 and \"R$\" in tag.text)[0].text # searching by text\n",
    "                prices.append(price)\n",
    "                location = d.find_all('img', src=\"./images/logo_location.png\")[0].next_sibling.strip()\n",
    "                locations.append(location)\n",
    "                bet_end_tokens = d.find_all(lambda tag: len(tag.find_all()) == 0 and \"Encerra\" in tag.text)[0].text\n",
    "                bet_end_date = self.__get_bet_end_date(bet_end_tokens)    \n",
    "                bet_end_dates.append(bet_end_date)\n",
    "                img_source = d.find_all('img')[0]['src']\n",
    "                img_sources.append(img_source)\n",
    "                popularity = d.find_all('img', src=\"./images/logo_visits.png\")[0].find_parent().text\n",
    "                popularities.append(popularity)\n",
    "\n",
    "            df = pd.DataFrame({'product_name': product_names, \n",
    "                               'price': prices, \n",
    "                               'url':urls, \n",
    "                               'location': locations, \n",
    "                               'bet_end_date': bet_end_dates, \n",
    "                               'img_source':img_sources, \n",
    "                               'popularity':popularities, \n",
    "                               'id':ids})\n",
    "            \n",
    "            if remove_duplicates:\n",
    "                df = df.drop_duplicates()\n",
    "            df['department'] = department\n",
    "            df['scraped_at'] = datetime.now().strftime('%Y-%m-%d %H:%M')\n",
    "            final_df = pd.concat([final_df, df])\n",
    "\n",
    "        return final_df\n",
    "          \n",
    "    \n",
    "    def facebook_marketplace_login(self, username:str, password:str) -> None:\n",
    "        \"\"\"Login steps for Facebook Marketplace.\n",
    "\n",
    "        Run through facebook login pages and insert login data.\n",
    "\n",
    "        Args:\n",
    "            username (str) : Username used to login at Facebook.\n",
    "            password (str) : Password used to login at Facebook.\n",
    "        \"\"\"\n",
    "        # find username/email field and send the username itself to the input field\n",
    "        self.wait.until(EC.element_to_be_clickable((By.XPATH, \"//input[@name='email']\"))).send_keys(username)\n",
    "        self.wait.until(EC.element_to_be_clickable((By.XPATH, \"//input[@name='pass']\"))).send_keys(password)\n",
    "        # click login button\n",
    "        self.wait.until(EC.element_to_be_clickable((By.XPATH, \"//button[@id='loginbutton']\"))).click()\n",
    "        \n",
    "    def facebook_marketplace_scrape(self, n_scrolls:int=5, remove_duplicates:bool=True) -> pd.DataFrame:\n",
    "        \"\"\"Scrape steps for Facebook Marketplace.\n",
    "\n",
    "        Run through facebook marketplace main results page and scrape data.\n",
    "\n",
    "        Args:\n",
    "            n_scrolls (int) : Number of ``PAGE DOWN`` button clicks to perform in the browser and load more items.\n",
    "            remove_duplicates (bool) : Whether to remove possible retrieved duplicated fields.\n",
    "            \n",
    "        Returns:\n",
    "            Retrieved data frame.\n",
    "        \"\"\"\n",
    "        # Wait for the page to load.\n",
    "        WebDriverWait(self.driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"div[class=' x1gslohp x1e56ztr']\")))\n",
    "        \n",
    "        query_filters_list = get_credentials('facebook')['query_filters']\n",
    "        df_final = pd.DataFrame()\n",
    "        for query_filters in query_filters_list:\n",
    "            department = query_filters['department']\n",
    "            print(f'...Querying Department = {department}...')\n",
    "            del query_filters['department'] # it's not part of the pars url\n",
    "            url = create_url_pars(f\"https://www.facebook.com/marketplace/category/{department}\", query_filters)\n",
    "            print(url)\n",
    "            self.driver.get(url) # force login, have to reenter url afterwards\n",
    "\n",
    "            # Wait for the page to load.\n",
    "            WebDriverWait(self.driver, 10).until(EC.presence_of_element_located(\n",
    "                (By.CSS_SELECTOR, \"div[class=' x1gslohp x1e56ztr']\")))\n",
    "\n",
    "            ActionChains(self.driver).send_keys(Keys.ESCAPE).perform() # test to dismiss notifications pop up\n",
    "\n",
    "            for i in range(1, n_scrolls):\n",
    "                # Scroll down to the bottom of the page to load all items.\n",
    "                self.driver.execute_script(\n",
    "                    \"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "                # Wait for the page to finish loading.\n",
    "                time.sleep(self.WAITING_TIME)\n",
    "\n",
    "            # Get the page source.\n",
    "            page_source = self.driver.page_source\n",
    "            soup = bs.BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "            # Get the items.\n",
    "            div = soup.find_all('div', class_='x9f619 x78zum5 x1r8uery xdt5ytf x1iyjqo2 xs83m0k x1e558r4 x150jy0e xnpuxes x291uyu x1uepa24 x1iorvi4 xjkvuk6')\n",
    "\n",
    "            # Iterate through the items.\n",
    "            images_src, titles, prices, urls, ids, locations = [],[],[],[],[],[]\n",
    "            for d in div:\n",
    "                # Get the item image.\n",
    "                img_ = d.find('img', class_='xt7dq6l xl1xv1r x6ikm8r x10wlt62 xh8yej3')\n",
    "                image = img_['src'] if img_ is not None else None\n",
    "                if img_ is not None:\n",
    "                    images_src.append(image)\n",
    "                    # Get the item title from span.\n",
    "                    title_ = d.find('span', 'x1lliihq x6ikm8r x10wlt62 x1n2onr6')\n",
    "                    title = title_.text if title_ is not None else None\n",
    "                    titles.append(title)\n",
    "                    # Get the item price. \n",
    "                    price_ = d.find('span', 'x193iq5w xeuugli x13faqbe x1vvkbs xlh3980 xvmahel x1n0sxbx x1lliihq x1s928wv xhkezso x1gmr53x x1cpjm7i x1fgarty x1943h6x x4zkp8e x3x7a5m x1lkfr7t x1lbecb7 x1s688f xzsf02u')\n",
    "                    price = price_.text if price_ is not None else None\n",
    "                    prices.append(price)\n",
    "                    # Get the item URL.\n",
    "                    url_ = d.find('a', class_='x1i10hfl xjbqb8w x6umtig x1b1mbwd xaqea5y xav7gou x9f619 x1ypdohk xt0psk2 xe8uvvx xdj266r x11i5rnm xat24cr x1mh8g0r xexx8yu x4uap5 x18d9i69 xkhd6sd x16tdsg8 x1hl2dhg xggy1nq x1a2a7pz x1heor9g x1lku1pv')\n",
    "                    url = url_['href'] if url_ is not None else None\n",
    "                    urls.append(f\"https://www.facebook.com{url}\")\n",
    "                    # get item id from url\n",
    "                    id_ = re.search('item/(\\d+)/', url).group(1)\n",
    "                    ids.append(id_)\n",
    "                    # Get the item location.\n",
    "                    location_ = d.find('span', 'x1lliihq x6ikm8r x10wlt62 x1n2onr6 xlyipyv xuxw1ft')\n",
    "                    location = location_.text if location_ is not None else None\n",
    "                    locations.append(location)\n",
    "            df = pd.DataFrame({'title': titles, \n",
    "                               'price': prices, \n",
    "                               'url':urls, \n",
    "                               'id': ids,\n",
    "                               'location':locations,\n",
    "                               'image_src': images_src})\n",
    "            if remove_duplicates:\n",
    "                df = df.drop_duplicates()\n",
    "            df['department'] = department    \n",
    "            df['scraped_at'] = datetime.now().strftime('%Y-%m-%d %H:%M')\n",
    "            df = self.facebook_marketplace_enrich(df)\n",
    "            df_final = pd.concat([df_final, df])\n",
    "        return df_final\n",
    "    \n",
    "    def facebook_marketplace_enrich(self, df:pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Add extra informations for the items retrieved in the scrape function.\n",
    "\n",
    "        Access each item page retrieved in the ``scrape`` function and extract more information.\n",
    "        Function needs the ``url`` column from the retrieved items.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame) : Data retrieved from Facebook Marketplace.\n",
    "            \n",
    "        Returns:\n",
    "            Enriched data frame.\n",
    "        \"\"\"\n",
    "        conditions, descriptions, times_posted = [], [],[]\n",
    "        for ix, row in df.iterrows():\n",
    "            if ix % 10 == 0:\n",
    "                print(f'Enriching {10 * (ix // 10) + 10} items out of {df.shape[0]}')\n",
    "            self.driver.get(row['url'])\n",
    "            try: # make sure everything is loaded by checking one of the text fields <-- avoid time.sleep conditions\n",
    "                self.wait.until(EC.presence_of_element_located((By.XPATH, \".//span[@class='x193iq5w xeuugli x13faqbe x1vvkbs xlh3980 xvmahel x1n0sxbx x6prxxf xvq8zen xo1l8bm xzsf02u']\")))\n",
    "            except Exception:\n",
    "                pass\n",
    "            page_source = self.driver.page_source\n",
    "            soup = bs.BeautifulSoup(page_source, 'html.parser')\n",
    "            condition = soup.find_all('span', class_='x193iq5w xeuugli x13faqbe x1vvkbs xlh3980 xvmahel x1n0sxbx x6prxxf xvq8zen xo1l8bm xzsf02u')\n",
    "            condition = condition[0].text if len(condition) > 0 else \"\"\n",
    "            conditions.append(condition)\n",
    "            description = soup.find_all('span', class_='x193iq5w xeuugli x13faqbe x1vvkbs xlh3980 xvmahel x1n0sxbx x1lliihq x1s928wv xhkezso x1gmr53x x1cpjm7i x1fgarty x1943h6x x4zkp8e x3x7a5m x6prxxf xvq8zen xo1l8bm xzsf02u')\n",
    "            description = description[1].text if len(description) > 0 else \"\"\n",
    "            descriptions.append(description)\n",
    "            time_posted = soup.find_all('span', class_='x193iq5w xeuugli x13faqbe x1vvkbs xlh3980 xvmahel x1n0sxbx x1lliihq x1s928wv xhkezso x1gmr53x x1cpjm7i x1fgarty x1943h6x x4zkp8e x676frb x1nxh6w3 x1sibtaa xo1l8bm xi81zsa')\n",
    "            time_posted = time_posted[0].text if len(time_posted) > 0 else \"\"\n",
    "            times_posted.append(time_posted)\n",
    "        df['condition'] = conditions\n",
    "        df['description'] = descriptions\n",
    "        df['time_posted'] = times_posted\n",
    "        return df\n",
    "\n",
    "    def twitter_login(self, username:str, password:str, otp_key:str):\n",
    "        \"\"\"Login steps for Twitter.\n",
    "\n",
    "        Run through Twitter login pages and insert login data.\n",
    "\n",
    "        Args:\n",
    "            username (str) : Username used to login at Twitter.\n",
    "            password (str) : Password used to login at Twitter.\n",
    "            otp_key (str) : One Time Password key.\n",
    "        \"\"\"\n",
    "        # find username/email field and send the username itself to the input field\n",
    "        self.wait.until(EC.element_to_be_clickable((By.XPATH, \"//input[@name='text']\"))).send_keys(username)\n",
    "        # click login button\n",
    "        self.wait.until(EC.element_to_be_clickable((By.XPATH, \"//div[@role='button'][contains(.,'Next')]\"))).click()\n",
    "\n",
    "        try:\n",
    "            # find password input field and insert password as well\n",
    "            self.wait.until(EC.element_to_be_clickable((By.XPATH, \"//input[@name='password']\"))).send_keys(password)\n",
    "            self.driver.find_element(By.XPATH, \"//div[@role='button'][contains(.,'Log in')]\").click()\n",
    "        except (TimeoutException, NoSuchElementException) as error:\n",
    "            # find username/email field and send the username itself to the input field\n",
    "            self.wait.until(EC.element_to_be_clickable((By.XPATH, \"//input[@name='text']\"))).send_keys('+5511949865898')\n",
    "            # click login button\n",
    "            self.wait.until(EC.element_to_be_clickable((By.XPATH, \"//div[@role='button'][contains(.,'Next')]\"))).click()\n",
    "            # find password input field and insert password as well\n",
    "            self.wait.until(EC.element_to_be_clickable((By.XPATH, \"//input[@name='password']\"))).send_keys(password)\n",
    "            self.wait.until(EC.element_to_be_clickable((By.XPATH, \"//div[@role='button'][contains(.,'Log in')]\"))).click()\n",
    "        finally:    \n",
    "            # finally add OTP\n",
    "            otp = totp = pyotp.TOTP(otp_key).now()\n",
    "            self.wait.until(EC.element_to_be_clickable((By.XPATH, \"//input[@name='text']\"))).send_keys(otp)\n",
    "            # login\n",
    "            self.wait.until(EC.element_to_be_clickable((By.XPATH, \"//div[@role='button'][contains(.,'Next')]\"))).click()\n",
    "            time.sleep(self.WAITING_TIME+2)\n",
    "        \n",
    "    def twitter_scrape(self, n_scrolls:int=5, remove_duplicates:bool=True):\n",
    "        \"\"\"Scrape steps for Twitter.\n",
    "\n",
    "        Run through Twitter main results page and scrape data.\n",
    "\n",
    "        Args:\n",
    "            n_scrolls (int) : Number of ``PAGE DOWN`` button clicks to perform in the browser and load more items.\n",
    "            remove_duplicates (bool) : Whether to remove possible retrieved duplicated fields.\n",
    "            \n",
    "        Returns:\n",
    "            Retrieved data frame.\n",
    "        \"\"\"\n",
    "    \n",
    "#         self.driver.get(\"https://twitter.com/home\")\n",
    "#         time.sleep(WAITING_TIME)\n",
    "\n",
    "        df_final = pd.DataFrame()\n",
    "        for i in range(n_scrolls):\n",
    "            # Get the page source.\n",
    "            page_source = self.driver.page_source\n",
    "            soup = bs.BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "            tweet_texts, authors_handle, posts_id, posts_url, datetimes_posted, languages = [], [], [], [], [], []\n",
    "            all_articles = soup.find_all('article')\n",
    "            for article in all_articles:\n",
    "                tweet_text = article.find_all('div', {'data-testid':'tweetText'})\n",
    "                tweet_text = tweet_text[0].text if len(tweet_text) > 0 else \"\"\n",
    "                tweet_texts.append(tweet_text)\n",
    "                author_handler = re.split('@|ยท', article.find_all('div', {'data-testid':'User-Name'})[0].text)[1]\n",
    "                authors_handle.append(author_handler)\n",
    "                post_id = ''\n",
    "                post_url = ''\n",
    "                for al in article.find_all('a'):\n",
    "                    url_match = re.match('.*status/\\d+$', al['href'])\n",
    "                    if url_match is not None:\n",
    "                        post_id = al['href'].split('/')[-1]\n",
    "                        post_url = f\"https://twitter.com{al['href']}\"\n",
    "                        break\n",
    "                posts_id.append(post_id)\n",
    "                posts_url.append(post_url)\n",
    "                language = article.find_all('div', {'data-testid':'tweetText'})\n",
    "                language = language[0]['lang'] if len(language) > 0 else \"\"\n",
    "                languages.append(language)\n",
    "                datetime_posted = article.find_all('time')\n",
    "                datetime_posted = datetime_posted[0]['datetime'] if len(datetime_posted) > 0 else None\n",
    "                datetimes_posted.append(datetime_posted)\n",
    "            df = pd.DataFrame({'id': posts_id,\n",
    "                               'url': posts_url,\n",
    "                               'tweet_text': tweet_texts, \n",
    "                               'author_handle': authors_handle, \n",
    "                               'datetime_posted': datetimes_posted, \n",
    "                               'language':languages})\n",
    "            df_final = pd.concat([df_final, df])\n",
    "    \n",
    "            for _ in range(3):\n",
    "                ActionChains(self.driver).send_keys(Keys.PAGE_DOWN).perform() \n",
    "                time.sleep(0.5)\n",
    "        df_final['tweet_text'] = df_final['tweet_text'].str.strip()\n",
    "        if remove_duplicates: # remove duplicates before adding scraping time\n",
    "            df_final = df_final.drop_duplicates(subset='id')\n",
    "        df_final['scraped_at'] = datetime.now().strftime('%Y-%m-%d %H:%M')\n",
    "        return df_final\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba6fd45",
   "metadata": {},
   "source": [
    "## Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "227123f0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Twitter Data...\n",
      "Saving and Updating File...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "ws = Webscraper(scrape_location=Webscraper.TWITTER)\n",
    "ws.login()\n",
    "ws.scrape(n_scrolls=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b640748",
   "metadata": {},
   "source": [
    "## Facebook Marketplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15510609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Facebook Marketplace Data...\n",
      "...Querying Department = furniture...\n",
      "https://www.facebook.com/marketplace/category/furniture?minPrice=10&maxPrice=200&daysSinceListed=1&sortBy=price_ascend\n",
      "Enriching 10 items out of 20\n",
      "Enriching 20 items out of 20\n",
      "...Querying Department = appliances...\n",
      "https://www.facebook.com/marketplace/category/appliances\n",
      "Enriching 10 items out of 20\n",
      "Enriching 20 items out of 20\n",
      "Saving and Updating File...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "ws = Webscraper(scrape_location=Webscraper.FACEBOOK_MP)\n",
    "ws.login()\n",
    "df = ws.scrape(n_scrolls=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2de8f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # change searching radius\n",
    "# ws.driver.find_element(By.XPATH, \"\"\"//div[@class='x78zum5 xl56j7k x1y1aw1k x1sxyh0 xwib8y2 xurb0ha']\"\"\").click()\n",
    "\n",
    "# radius_button = ws.driver.find_element(By.XPATH, \"\"\"//div[@class='xjbqb8w x1iyjqo2 x193iq5w xeuugli x1n2onr6'][contains(.,'Radius')]\"\"\")\n",
    "# radius_button.click()\n",
    "# # time.sleep(.5)\n",
    "\n",
    "# # id changes, take a look after\n",
    "# # ws.driver.find_element(By.XPATH, \"\"\".//div[@class='x1i10hfl xjbqb8w x6umtig x1b1mbwd xaqea5y xav7gou xe8uvvx x1hl2dhg xggy1nq x1o1ewxj x3x9cwd x1e5q0jg x13rtm0m x87ps6o x1lku1pv x1a2a7pz x6s0dn4 xjyslct x9f619 x1ypdohk x78zum5 x1q0g3np x2lah0s xnqzcj9 x1gh759c xdj266r xat24cr x1344otq x1de53dj xz9dl7a xsag5q8 x1n2onr6 x16tdsg8 x1ja2u2z' and @id=':r3b:__5']\"\"\").click()\n",
    "# # create equivalences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fec066",
   "metadata": {},
   "source": [
    "## Superbid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb61908a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Superbid Data...\n",
      "...Querying Department = eletrodomesticos...\n",
      "...Querying Department = appliances...\n",
      "...Querying Department = artes-decoracao-colecionismo...\n",
      "Saving and Updating File...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "ws = Webscraper(scrape_location=Webscraper.SUPERBID)\n",
    "df = ws.scrape()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe75950e",
   "metadata": {},
   "source": [
    "## Running as modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6cf3508",
   "metadata": {},
   "outputs": [],
   "source": [
    "from webscraper import Webscraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8389befc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Superbid Data...\n",
      "...Querying Department = eletrodomesticos...\n",
      "...Querying Department = appliances...\n",
      "...Querying Department = artes-decoracao-colecionismo...\n",
      "Saving and Updating File...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "ws = Webscraper(Webscraper.SUPERBID)\n",
    "ws.scrape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cb614d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
